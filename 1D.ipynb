{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install wfdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: deepdish in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (0.3.6)\n",
      "Requirement already satisfied: scipy in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from deepdish) (1.4.1)\n",
      "Requirement already satisfied: numpy in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from deepdish) (1.17.2)\n",
      "Requirement already satisfied: tables in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from deepdish) (3.5.2)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from tables->deepdish) (1.12.0)\n",
      "Requirement already satisfied: numexpr>=2.6.2 in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from tables->deepdish) (2.7.0)\n",
      "Requirement already satisfied: mock>=2.0 in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from tables->deepdish) (3.0.5)\n"
     ]
    }
   ],
   "source": [
    "! pip install deepdish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "\n",
    "import deepdish as dd\n",
    "import deepdish.io as ddio\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import wfdb\n",
    "\n",
    "from glob import glob\n",
    "from scipy.signal import find_peaks\n",
    "from sklearn import preprocessing\n",
    "from tqdm import tqdm\n",
    "from wfdb import rdrecord, rdann\n",
    "from scipy.io import loadmat\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Input, Conv1D, Dense, add, Dropout, MaxPooling1D, Activation, BatchNormalization, Lambda\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is provided by\n",
    "https://physionet.org/physiobank/database/html/mitdbdir/mitdbdir.htm\n",
    "\n",
    "The recordings were digitized at 360 samples per second per channel with 11-bit resolution over a 10 mV range.\n",
    "Two or more cardiologists independently annotated each record; disagreements were resolved to obtain the computer-readable\n",
    "reference annotations for each beat (approximately 110,000 annotations in all) included with the database.\n",
    "\n",
    "    Code\t\tDescription\n",
    "    N\t\tNormal beat (displayed as . by the PhysioBank ATM, LightWAVE, pschart, and psfd)\n",
    "    L\t\tLeft bundle branch block beat\n",
    "    R\t\tRight bundle branch block beat\n",
    "    B\t\tBundle branch block beat (unspecified)\n",
    "    A\t\tAtrial premature beat\n",
    "    a\t\tAberrated atrial premature beat\n",
    "    J\t\tNodal (junctional) premature beat\n",
    "    S\t\tSupraventricular premature or ectopic beat (atrial or nodal)\n",
    "    V\t\tPremature ventricular contraction\n",
    "    r\t\tR-on-T premature ventricular contraction\n",
    "    F\t\tFusion of ventricular and normal beat\n",
    "    e\t\tAtrial escape beat\n",
    "    j\t\tNodal (junctional) escape beat\n",
    "    n\t\tSupraventricular escape beat (atrial or nodal)\n",
    "    E\t\tVentricular escape beat\n",
    "    /\t\tPaced beat\n",
    "    f\t\tFusion of paced and normal beat\n",
    "    Q\t\tUnclassifiable beat\n",
    "    ?\t\tBeat not classified during learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You already have the data.\n"
     ]
    }
   ],
   "source": [
    "if os.path.isdir(\"mitdb\"):\n",
    "    print('You already have the data.')\n",
    "else:\n",
    "    wfdb.dl_database('mitdb', 'mitdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_records():\n",
    "    paths = glob('mitdb/*.atr')\n",
    "    paths = [path[:-4].rsplit(\"/\", 1)[1] for path in paths]\n",
    "    paths.sort()\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 256\n",
    "features = ['MLII', 'V1', 'V2', 'V4', 'V5']  # signal names\n",
    "nums = get_records()                         # file names without extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data():\n",
    "    test_set = ['101', '105', '114', '118', '124', '201', '210', '217']\n",
    "    train_set = [x for x in nums if x not in test_set]\n",
    "    data_saver(train_set, 'mitdb/train.hdf5', 'mitdb/trainlabel.hdf5')\n",
    "    data_saver(test_set, 'mitdb/test.hdf5', 'mitdb/testlabel.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_saver(dataset, dataset_name, labels_name):\n",
    "    classes = ['N', 'V', '/', 'A', 'F', '~']\n",
    "    classes_length = len(classes)  # used for creating masks\n",
    "    datadict, data_label = dict(), dict()\n",
    "\n",
    "    \"\"\"\n",
    "    {\n",
    "        \"N\": [],\n",
    "        \"V\": [],\n",
    "        \"/\": [],\n",
    "        \"A\": [],\n",
    "        \"F\": [],\n",
    "        \"~\": []\n",
    "    }\n",
    "    \"\"\"\n",
    "    for feature in features:\n",
    "        datadict[feature] = list()\n",
    "        data_label[feature] = list()\n",
    "\n",
    "    data_process(dataset, classes, datadict, data_label)\n",
    "    add_noise_to_dataset(classes_length, data_label, datadict)\n",
    "\n",
    "    dd.io.save(dataset_name, datadict)\n",
    "    dd.io.save(labels_name, data_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(dataset, classes, datadict, data_label):\n",
    "    for num in tqdm(dataset):\n",
    "        record = rdrecord('mitdb/' + num, smooth_frames=True)\n",
    "\n",
    "        signals_channel_0 = preprocessing.scale(np.nan_to_num(record.p_signal[:, 0])).tolist()\n",
    "        signals_channel_1 = preprocessing.scale(np.nan_to_num(record.p_signal[:, 1])).tolist()\n",
    "\n",
    "        peaks, _ = find_peaks(signals_channel_0, distance=150)\n",
    "\n",
    "        feature0, feature1 = record.sig_name[0], record.sig_name[1]\n",
    "\n",
    "        # skip the first and last peaks to have enough range of the sample\n",
    "        for peak in peaks[1:-1]:\n",
    "            start, end = peak - INPUT_SIZE // 2, peak + INPUT_SIZE // 2\n",
    "            annotation = rdann('mitdb/' + num, extension='atr', sampfrom=start, sampto=end, return_label_elements=['symbol'])\n",
    "\n",
    "            # remove some of \"N\" which breaks the balance of dataset\n",
    "            if len(annotation.symbol) == 1 and (annotation.symbol[0] in classes) and (annotation.symbol[0] != \"N\" or np.random.random() < 0.15):\n",
    "                y = [0] * len(classes)\n",
    "                y[classes.index(annotation.symbol[0])] = 1\n",
    "                data_label[feature0].append(y)\n",
    "                data_label[feature1].append(y)\n",
    "                datadict[feature0].append(signals_channel_0[start:end])\n",
    "                datadict[feature1].append(signals_channel_1[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise_to_dataset(classes_length, data_label, datadict):\n",
    "        noises = add_noise()\n",
    "        for feature in [\"MLII\", \"V1\"]:\n",
    "            d = np.array(datadict[feature])\n",
    "            if len(d) > 15 * 10 ** 3:\n",
    "                n = np.array(noises[\"trainset\"])\n",
    "            else:\n",
    "                n = np.array(noises[\"testset\"])\n",
    "\n",
    "            datadict[feature] = np.concatenate((d, n))\n",
    "            size, _ = n.shape\n",
    "            l = np.array(data_label[feature])\n",
    "            noise_label = [0] * classes_length\n",
    "            noise_label[-1] = 1\n",
    "\n",
    "            noise_label = np.array([noise_label] * size)\n",
    "            data_label[feature] = np.concatenate((l, noise_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise():\n",
    "    noises = dict()\n",
    "    noises[\"trainset\"] = list()\n",
    "    noises[\"testset\"] = list()\n",
    "\n",
    "    try:\n",
    "        testlabel = list(csv.reader(open('training2017/REFERENCE.csv')))\n",
    "    except:\n",
    "        cmd = \"curl -O https://archive.physionet.org/challenge/2017/training2017.zip\"\n",
    "        os.system(cmd)\n",
    "        os.system(\"unzip training2017.zip\")\n",
    "        testlabel = list(csv.reader(open('training2017/REFERENCE.csv')))\n",
    "    for i, label in enumerate(testlabel):\n",
    "        if label[1] == '~':\n",
    "            filename = 'training2017/' + label[0] + '.mat'\n",
    "            from scipy.io import loadmat\n",
    "            noise = loadmat(filename)\n",
    "            noise = noise['val']\n",
    "            _, size = noise.shape\n",
    "            noise = noise.reshape(size, )\n",
    "            noise = np.nan_to_num(noise)  # removing NaNs and Infs\n",
    "            from scipy.signal import resample\n",
    "            noise = resample(noise, int(\n",
    "                len(noise) * 360 / 300))  # resample to match the data sampling rate 360(mit), 300(cinc)\n",
    "            from sklearn import preprocessing\n",
    "            noise = preprocessing.scale(noise)\n",
    "            noise = noise / 1000 * 6  # rough normalize, to be improved\n",
    "            from scipy.signal import find_peaks\n",
    "            peaks, _ = find_peaks(noise, distance=150)\n",
    "            choices = 10  # 256*10 from 9000\n",
    "            picked_peaks = np.random.choice(peaks, choices, replace=False)\n",
    "            for j, peak in enumerate(picked_peaks):\n",
    "                if peak > INPUT_SIZE // 2 and peak < len(noise) - INPUT_SIZE // 2:\n",
    "                    start, end = peak - INPUT_SIZE // 2, peak + INPUT_SIZE // 2\n",
    "                    if i > len(testlabel) / 6:\n",
    "                        noises[\"trainset\"].append(noise[start:end].tolist())\n",
    "                    else:\n",
    "                        noises[\"testset\"].append(noise[start:end].tolist())\n",
    "    return noises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loaddata(input_size, feature):\n",
    "    trainData = ddio.load('mitdb/train.hdf5')\n",
    "    testlabelData = ddio.load('mitdb/trainlabel.hdf5')\n",
    "    X = np.float32(trainData[feature])\n",
    "    print(\"[+] X:\")\n",
    "    print(X)\n",
    "    y = np.float32(testlabelData[feature])\n",
    "    print(\"[+] y:\")\n",
    "    print(y)\n",
    "    att = np.concatenate((X, y), axis=1)\n",
    "    print(\"[+] Att:\")\n",
    "    print(att)\n",
    "    np.random.shuffle(att)\n",
    "    X, y = att[:, :input_size], att[:, input_size:]\n",
    "    valData = ddio.load('mitdb/test.hdf5')\n",
    "    vallabelData = ddio.load('mitdb/testlabel.hdf5')\n",
    "    Xval = np.float32(valData[feature])\n",
    "    yval = np.float32(vallabelData[feature])\n",
    "    return (X, y, Xval, yval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_recursive(path):\n",
    "    if path == \"\":\n",
    "        return\n",
    "    sub_path = os.path.dirname(path)\n",
    "    if not os.path.exists(sub_path):\n",
    "        mkdir_recursive(sub_path)\n",
    "    if not os.path.exists(path):\n",
    "        print(\"Creating directory \" + path)\n",
    "        os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PR_ROC_curves(ytrue, ypred, classes, ypred_mat):\n",
    "    ybool = ypred == ytrue\n",
    "    f, ax = plt.subplots(3, 4, figsize=(10, 10))\n",
    "    ax = [a for i in ax for a in i]\n",
    "\n",
    "    e = -1\n",
    "    for c in classes:\n",
    "        idx1 = [n for n, x in enumerate(ytrue) if classes[x] == c]\n",
    "        idx2 = [n for n, x in enumerate(ypred) if classes[x] == c]\n",
    "        idx = idx1 + idx2\n",
    "        if idx == []:\n",
    "            continue\n",
    "        bi_ytrue = ytrue[idx]\n",
    "        bi_prob = ypred_mat[idx, :]\n",
    "        bi_ybool = np.array(ybool[idx])\n",
    "        bi_yscore = np.array([bi_prob[x][bi_ytrue[x]] for x in range(len(idx))])\n",
    "        try:\n",
    "            print(\"AUC for {}: {}\".format(c, roc_auc_score(bi_ybool + 0, bi_yscore)))\n",
    "            e += 1\n",
    "        except ValueError:\n",
    "            continue\n",
    "        ppvs, senss, thresholds = precision_recall_curve(bi_ybool, bi_yscore)\n",
    "        cax = ax[2 * e]\n",
    "        cax.plot(ppvs, senss, lw=2, label=\"Model\")\n",
    "        cax.set_xlim(-0.008, 1.05)\n",
    "        cax.set_ylim(0.0, 1.05)\n",
    "        cax.set_title(\"Class {}\".format(c))\n",
    "        cax.set_xlabel('Sensitivity (Recall)')\n",
    "        cax.set_ylabel('PPV (Precision)')\n",
    "        cax.legend(loc=3)\n",
    "\n",
    "        fpr, tpr, thresholds = roc_curve(bi_ybool, bi_yscore)\n",
    "        cax2 = ax[2 * e + 1]\n",
    "        cax2.plot(fpr, tpr, lw=2, label=\"Model\")\n",
    "        cax2.set_xlim(-0.1, 1.)\n",
    "        cax2.set_ylim(0.0, 1.05)\n",
    "        cax2.set_title(\"Class {}\".format(c))\n",
    "        cax2.set_xlabel('1 - Specificity')\n",
    "        cax2.set_ylabel('Sensitivity')\n",
    "        cax2.legend(loc=4)\n",
    "\n",
    "    mkdir_recursive(\"results\")\n",
    "    plt.savefig(\"results/model_prec_recall_and_roc.eps\",\n",
    "                dpi=400,\n",
    "                format='eps',\n",
    "                bbox_inches='tight')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes, feature,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # classes = classes[unique_labels(y_true, y_pred)]\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    mkdir_recursive('results')\n",
    "    fig.savefig('results/confusionMatrix-mlii.eps', format='eps', dpi=1000)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(model, Xval, yval, classes):\n",
    "    model2 = model\n",
    "    model.load_weights('models/MLII-latest.hdf5')\n",
    "    # to combine different trained models. On testing\n",
    "    \n",
    "    ypred_mat = model.predict(Xval)\n",
    "    \n",
    "    ypred_mat = ypred_mat[:, 0]\n",
    "    yval = yval[:, 0]\n",
    "\n",
    "    ytrue = np.argmax(yval, axis=1)\n",
    "    yscore = np.array([ypred_mat[x][ytrue[x]] for x in range(len(yval))])\n",
    "    ypred = np.argmax(ypred_mat, axis=1)\n",
    "    print(classification_report(ytrue, ypred))\n",
    "    plot_confusion_matrix(ytrue, ypred, classes, feature=32, normalize=False)\n",
    "    print(\"F1 score:\", f1_score(ytrue, ypred, average=None))\n",
    "    PR_ROC_curves(ytrue, ypred, classes, ypred_mat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EcgModel(object):\n",
    "    def first_convolution_block(self, inputs):\n",
    "        layer = Conv1D(filters=32,\n",
    "                       kernel_size=16,\n",
    "                       padding='same',\n",
    "                       strides=1,\n",
    "                       kernel_initializer='he_normal')(inputs)\n",
    "        layer = BatchNormalization()(layer)\n",
    "        layer = Activation('relu')(layer)\n",
    "\n",
    "        shortcut = MaxPooling1D(pool_size=1,\n",
    "                                strides=1)(layer)\n",
    "\n",
    "        layer = Conv1D(filters=32,\n",
    "                       kernel_size=16,\n",
    "                       padding='same',\n",
    "                       strides=1,\n",
    "                       kernel_initializer='he_normal')(layer)\n",
    "        layer = BatchNormalization()(layer)\n",
    "        layer = Activation('relu')(layer)\n",
    "        layer = Dropout(0.2)(layer)\n",
    "        layer = Conv1D(filters=32,\n",
    "                       kernel_size=16,\n",
    "                       padding='same',\n",
    "                       strides=1,\n",
    "                       kernel_initializer='he_normal')(layer)\n",
    "        return add([shortcut, layer])\n",
    "\n",
    "    def main_loop_blocks(self, layer):\n",
    "        filter_length = 32\n",
    "        n_blocks = 15\n",
    "        for block_index in range(n_blocks):\n",
    "            def zeropad(x):\n",
    "                y = K.zeros_like(x)\n",
    "                return K.concatenate([x, y], axis=2)\n",
    "\n",
    "            def zeropad_output_shape(input_shape):\n",
    "                shape = list(input_shape)\n",
    "                assert len(shape) == 3\n",
    "                shape[2] *= 2\n",
    "                return tuple(shape)\n",
    "\n",
    "            subsample_length = 2 if block_index % 2 == 0 else 1\n",
    "            shortcut = MaxPooling1D(pool_size=subsample_length)(layer)\n",
    "\n",
    "            # 5 is chosen instead of 4 from the original model\n",
    "            if block_index % 4 == 0 and block_index > 0:\n",
    "                # double size of the network and match the shapes of both branches\n",
    "                shortcut = Lambda(zeropad, output_shape=zeropad_output_shape)(shortcut)\n",
    "                filter_length *= 2\n",
    "\n",
    "            layer = BatchNormalization()(layer)\n",
    "            layer = Activation('relu')(layer)\n",
    "            layer = Conv1D(filters=filter_length,\n",
    "                           kernel_size=16,\n",
    "                           padding='same',\n",
    "                           strides=subsample_length,\n",
    "                           kernel_initializer='he_normal')(layer)\n",
    "            layer = BatchNormalization()(layer)\n",
    "            layer = Activation('relu')(layer)\n",
    "            layer = Dropout(0.2)(layer)\n",
    "            layer = Conv1D(filters=filter_length,\n",
    "                           kernel_size=16,\n",
    "                           padding='same',\n",
    "                           strides=1,\n",
    "                           kernel_initializer='he_normal')(layer)\n",
    "            layer = add([shortcut, layer])\n",
    "        return layer\n",
    "\n",
    "    def output_block(self, layer, inputs):\n",
    "        classes = ['N', 'V', '/', 'A', 'F', '~']\n",
    "        len_classes = len(classes)\n",
    "        layer = BatchNormalization()(layer)\n",
    "        layer = Activation('relu')(layer)\n",
    "        outputs = TimeDistributed(Dense(len_classes, activation='softmax'))(layer)\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def create_network(self):\n",
    "        inputs = Input(shape=(256, 1), name='input')\n",
    "        layer = self.first_convolution_block(inputs)\n",
    "        layer = self.main_loop_blocks(layer)\n",
    "        return self.output_block(layer, inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train(object):\n",
    "    def __init__(self):\n",
    "        mkdir_recursive('models')\n",
    "\n",
    "    @staticmethod\n",
    "    def run(X, y, Xval=None, yval=None):\n",
    "        classes = ['N', 'V', '/', 'A', 'F', '~']\n",
    "\n",
    "        x = np.expand_dims(X, axis=2)\n",
    "\n",
    "        \n",
    "        xv = np.expand_dims(Xval, axis=2)\n",
    "        (m, n) = y.shape\n",
    "\n",
    "        print(\"-----\")\n",
    "        print(m)\n",
    "        print(n)\n",
    "        print(\"-----\")\n",
    "\n",
    "        y = y.reshape((m, 1, n))\n",
    "        (mvl, nvl) = yval.shape\n",
    "        yv = yval.reshape((mvl, 1, nvl))\n",
    "\n",
    "        model = EcgModel().create_network()\n",
    "\n",
    "        callbacks = [\n",
    "            EarlyStopping(patience=10, verbose=1),\n",
    "            ReduceLROnPlateau(factor=0.5, patience=3, min_lr=0.01, verbose=1),\n",
    "            TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_grads=False, write_images=True),\n",
    "            ModelCheckpoint('models/{}-latest.hdf5'.format(\"MLII\"), monitor='val_loss', save_best_only=False, verbose=1, period=10)\n",
    "        ]\n",
    "\n",
    "        if model is None:\n",
    "            print(\"none\")\n",
    "\n",
    "        model.fit(x, y,\n",
    "                  validation_data=(xv, yv),\n",
    "                  epochs=100,\n",
    "                  batch_size=256,\n",
    "                  callbacks=callbacks,\n",
    "                  initial_epoch=0)\n",
    "        print_results(model, xv, yv, classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] X:\n",
      "[[-4.0735614e-01 -3.8147619e-01 -3.2971624e-01 ... -7.0916437e-02\n",
      "  -4.5036457e-02 -1.9156480e-02]\n",
      " [-2.5207630e-01 -2.5207630e-01 -2.0031632e-01 ...  1.1024341e-01\n",
      "   1.1024341e-01  1.8788335e-01]\n",
      " [ 6.7234989e-03  3.2603476e-02  1.1024341e-01 ...  1.8788335e-01\n",
      "   2.6552328e-01  2.1376333e-01]\n",
      " ...\n",
      " [ 5.6652667e-04  5.6255813e-04  5.7126599e-04 ... -9.0843486e-03\n",
      "  -9.6476022e-03 -9.9608470e-03]\n",
      " [-4.3222876e-03 -2.8862390e-03 -4.1278726e-03 ...  6.9519258e-03\n",
      "   7.9615498e-03  6.7522442e-03]\n",
      " [-1.1040693e-04 -1.2917866e-04 -1.6406785e-04 ... -2.7200853e-04\n",
      "  -3.0427764e-04 -3.2162119e-04]]\n",
      "[+] y:\n",
      "[[0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]]\n",
      "[+] Att:\n",
      "[[-4.0735614e-01 -3.8147619e-01 -3.2971624e-01 ...  1.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]\n",
      " [-2.5207630e-01 -2.5207630e-01 -2.0031632e-01 ...  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]\n",
      " [ 6.7234989e-03  3.2603476e-02  1.1024341e-01 ...  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]\n",
      " ...\n",
      " [ 5.6652667e-04  5.6255813e-04  5.7126599e-04 ...  0.0000000e+00\n",
      "   0.0000000e+00  1.0000000e+00]\n",
      " [-4.3222876e-03 -2.8862390e-03 -4.1278726e-03 ...  0.0000000e+00\n",
      "   0.0000000e+00  1.0000000e+00]\n",
      " [-1.1040693e-04 -1.2917866e-04 -1.6406785e-04 ...  0.0000000e+00\n",
      "   0.0000000e+00  1.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "(X, y, Xval, yval) = loaddata(256, \"MLII\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train().run(X, y, Xval, yval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cincData():\n",
    "    __download_cinc_data()\n",
    "\n",
    "    testlabel = []\n",
    "    __read_data(testlabel)\n",
    "\n",
    "    high = len(testlabel) - 1\n",
    "    num = np.random.randint(1, high)\n",
    "    filename, label = testlabel[num - 1]\n",
    "    filename = 'training2017/' + filename + '.mat'\n",
    "\n",
    "    data = loadmat(filename)\n",
    "    print(\"The record of \" + filename)\n",
    "    data = data['val']\n",
    "    _, size = data.shape\n",
    "    data = data.reshape(size, )\n",
    "    \n",
    "    return data, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __download_cinc_data():\n",
    "    cmd = \"curl -O https://archive.physionet.org/challenge/2017/training2017.zip\"\n",
    "    os.system(cmd)\n",
    "    os.system(\"unzip training2017.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __read_data(testlabel):\n",
    "    with open('training2017/REFERENCE.csv') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            testlabel.append([row[0], row[1]])\n",
    "            line_count += 1\n",
    "        print(f'Processed {line_count} lines.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, label, peaks):\n",
    "    classesM = ['N', 'Ventricular', 'Paced', 'A', 'F', 'Noise']\n",
    "    predicted, result = predictByPart(data, peaks)\n",
    "    sumPredict = sum(predicted[x][1] for x in range(len(predicted)))\n",
    "    avgPredict = sumPredict / len(predicted)\n",
    "    print(\"The average of the predict is:\", avgPredict)\n",
    "    print(\"The most predicted label is {} with {:3.1f}% certainty\".format(classesM[avgPredict.argmax()],\n",
    "                                                                          100 * max(avgPredict[0])))\n",
    "    sec_idx = avgPredict.argsort()[0][-2]\n",
    "    print(\"The second predicted label is {} with {:3.1f}% certainty\".format(classesM[sec_idx],\n",
    "                                                                            100 * avgPredict[0][sec_idx]))\n",
    "    print(\"The original label of the record is \" + label)\n",
    "    print(\"Result:\")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictByPart(data, peaks):\n",
    "    classesM = ['N', 'Ventricular', 'Paced', 'A', 'F', 'Noise']\n",
    "    predicted = list()\n",
    "    result = \"\"\n",
    "    counter = [0] * len(classesM)\n",
    "\n",
    "    model = load_model('models/MLII-latest.hdf5')\n",
    "    \n",
    "    for i, peak in enumerate(peaks[3:-1]):\n",
    "        total_n = len(peaks)\n",
    "        start, end = peak - INPUT_SIZE // 2, peak + INPUT_SIZE // 2\n",
    "        prob = model.predict(data[:, start:end])\n",
    "        prob = prob[:, 0]\n",
    "        ann = np.argmax(prob)\n",
    "        counter[ann] += 1\n",
    "        if classesM[ann] != \"N\":\n",
    "            print(\"The {}/{}-record classified as {} with {:3.1f}% certainty\".format(i, total_n, classesM[ann], 100 * prob[0, ann]))\n",
    "        result += \"(\" + classesM[ann] + \":\" + str(round(100 * prob[0, ann], 1)) + \"%)\"\n",
    "        predicted.append([classesM[ann], prob])\n",
    "        if classesM[ann] != 'N' and prob[0, ann] > 0.95:\n",
    "            import matplotlib.pyplot as plt\n",
    "            plt.plot(data[:, start:end][0, :, 0], )\n",
    "            mkdir_recursive('results')\n",
    "            plt.savefig('results/hazard-' + classesM[ann] + '.png', format=\"png\", dpi=300)\n",
    "            plt.close()\n",
    "    result += \"{}-N, {}-Venticular, {}-Paced, {}-A, {}-F, {}-Noise\".format(counter[0], counter[1], counter[2], counter[3], counter[4], counter[5])\n",
    "    return predicted, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 8528 lines.\n",
      "The record of training2017/A00256.mat\n"
     ]
    }
   ],
   "source": [
    "data, label = cincData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import resample\n",
    "from sklearn import preprocessing\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(data):\n",
    "    sr = 300\n",
    "    data = np.nan_to_num(data)  # removing NaNs and Infs\n",
    "    data = resample(data, int(len(data) * 360 / sr))  # resample to match the data sampling rate 360(mit), 300(cinc)\n",
    "    data = preprocessing.scale(data)\n",
    "    peaks, _ = find_peaks(data, distance=150)\n",
    "    data = data.reshape(1, len(data))\n",
    "    data = np.expand_dims(data, axis=2)  # required by Keras\n",
    "    return data, peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, peaks = preprocess(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 1/50-record classified as Ventricular with 99.6% certainty\n",
      "The 3/50-record classified as Ventricular with 99.6% certainty\n",
      "The 11/50-record classified as Ventricular with 99.9% certainty\n",
      "The 12/50-record classified as Ventricular with 90.7% certainty\n",
      "The 14/50-record classified as Noise with 71.2% certainty\n",
      "The 17/50-record classified as Ventricular with 74.2% certainty\n",
      "The 23/50-record classified as Ventricular with 99.8% certainty\n",
      "The 32/50-record classified as Ventricular with 99.8% certainty\n",
      "The 34/50-record classified as Ventricular with 99.7% certainty\n",
      "The 36/50-record classified as Ventricular with 99.4% certainty\n",
      "The 38/50-record classified as Ventricular with 99.0% certainty\n",
      "The 44/50-record classified as Ventricular with 99.9% certainty\n",
      "The average of the predict is: [[0.6705503  0.29251155 0.00073118 0.00385717 0.01139939 0.02095031]]\n",
      "The most predicted label is N with 67.1% certainty\n",
      "The second predicted label is Ventricular with 29.3% certainty\n",
      "The original label of the record is N\n",
      "Result:\n",
      "(N:99.4%)(Ventricular:99.6%)(N:99.7%)(Ventricular:99.6%)(N:77.3%)(N:99.8%)(N:98.1%)(N:78.8%)(N:98.9%)(N:99.0%)(N:92.6%)(Ventricular:99.9%)(Ventricular:90.7%)(N:57.0%)(Noise:71.2%)(N:100.0%)(N:98.2%)(Ventricular:74.2%)(N:91.6%)(N:99.3%)(N:91.3%)(N:97.1%)(N:86.7%)(Ventricular:99.8%)(N:99.3%)(N:74.4%)(N:86.2%)(N:99.1%)(N:92.7%)(N:87.6%)(N:99.6%)(N:87.4%)(Ventricular:99.8%)(N:99.7%)(Ventricular:99.7%)(N:99.4%)(Ventricular:99.4%)(N:99.4%)(Ventricular:99.0%)(N:51.3%)(N:97.3%)(N:77.4%)(N:98.6%)(N:94.4%)(Ventricular:99.9%)(N:63.8%)34-N, 11-Venticular, 0-Paced, 0-A, 0-F, 1-Noise\n"
     ]
    }
   ],
   "source": [
    "predict(data, label, peaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 1/50-record classified as Ventricular with 99.6% certainty\n",
      "The 3/50-record classified as Ventricular with 99.6% certainty\n",
      "The 11/50-record classified as Ventricular with 99.9% certainty\n",
      "The 12/50-record classified as Ventricular with 90.7% certainty\n",
      "The 14/50-record classified as Noise with 71.2% certainty\n",
      "The 17/50-record classified as Ventricular with 74.2% certainty\n",
      "The 23/50-record classified as Ventricular with 99.8% certainty\n",
      "The 32/50-record classified as Ventricular with 99.8% certainty\n",
      "The 34/50-record classified as Ventricular with 99.7% certainty\n",
      "The 36/50-record classified as Ventricular with 99.4% certainty\n",
      "The 38/50-record classified as Ventricular with 99.0% certainty\n",
      "The 44/50-record classified as Ventricular with 99.9% certainty\n",
      "The average of the predict is: [[0.6705503  0.29251155 0.00073118 0.00385717 0.01139939 0.02095031]]\n",
      "The most predicted label is N with 67.1% certainty\n",
      "The second predicted label is Ventricular with 29.3% certainty\n",
      "The original label of the record is N\n",
      "Result:\n",
      "(N:99.4%)(Ventricular:99.6%)(N:99.7%)(Ventricular:99.6%)(N:77.3%)(N:99.8%)(N:98.1%)(N:78.8%)(N:98.9%)(N:99.0%)(N:92.6%)(Ventricular:99.9%)(Ventricular:90.7%)(N:57.0%)(Noise:71.2%)(N:100.0%)(N:98.2%)(Ventricular:74.2%)(N:91.6%)(N:99.3%)(N:91.3%)(N:97.1%)(N:86.7%)(Ventricular:99.8%)(N:99.3%)(N:74.4%)(N:86.2%)(N:99.1%)(N:92.7%)(N:87.6%)(N:99.6%)(N:87.4%)(Ventricular:99.8%)(N:99.7%)(Ventricular:99.7%)(N:99.4%)(Ventricular:99.4%)(N:99.4%)(Ventricular:99.0%)(N:51.3%)(N:97.3%)(N:77.4%)(N:98.6%)(N:94.4%)(Ventricular:99.9%)(N:63.8%)34-N, 11-Venticular, 0-Paced, 0-A, 0-F, 1-Noise\n"
     ]
    }
   ],
   "source": [
    "predict(data, label, peaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 1/50-record classified as Ventricular with 99.6% certainty\n",
      "The 3/50-record classified as Ventricular with 99.6% certainty\n",
      "The 11/50-record classified as Ventricular with 99.9% certainty\n",
      "The 12/50-record classified as Ventricular with 90.7% certainty\n",
      "The 14/50-record classified as Noise with 71.2% certainty\n",
      "The 17/50-record classified as Ventricular with 74.2% certainty\n",
      "The 23/50-record classified as Ventricular with 99.8% certainty\n",
      "The 32/50-record classified as Ventricular with 99.8% certainty\n",
      "The 34/50-record classified as Ventricular with 99.7% certainty\n",
      "The 36/50-record classified as Ventricular with 99.4% certainty\n",
      "The 38/50-record classified as Ventricular with 99.0% certainty\n",
      "The 44/50-record classified as Ventricular with 99.9% certainty\n",
      "The average of the predict is: [[0.6705503  0.29251155 0.00073118 0.00385717 0.01139939 0.02095031]]\n",
      "The most predicted label is N with 67.1% certainty\n",
      "The second predicted label is Ventricular with 29.3% certainty\n",
      "The original label of the record is N\n",
      "Result:\n",
      "(N:99.4%)(Ventricular:99.6%)(N:99.7%)(Ventricular:99.6%)(N:77.3%)(N:99.8%)(N:98.1%)(N:78.8%)(N:98.9%)(N:99.0%)(N:92.6%)(Ventricular:99.9%)(Ventricular:90.7%)(N:57.0%)(Noise:71.2%)(N:100.0%)(N:98.2%)(Ventricular:74.2%)(N:91.6%)(N:99.3%)(N:91.3%)(N:97.1%)(N:86.7%)(Ventricular:99.8%)(N:99.3%)(N:74.4%)(N:86.2%)(N:99.1%)(N:92.7%)(N:87.6%)(N:99.6%)(N:87.4%)(Ventricular:99.8%)(N:99.7%)(Ventricular:99.7%)(N:99.4%)(Ventricular:99.4%)(N:99.4%)(Ventricular:99.0%)(N:51.3%)(N:97.3%)(N:77.4%)(N:98.6%)(N:94.4%)(Ventricular:99.9%)(N:63.8%)34-N, 11-Venticular, 0-Paced, 0-A, 0-F, 1-Noise\n"
     ]
    }
   ],
   "source": [
    "predict(data, label, peaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
